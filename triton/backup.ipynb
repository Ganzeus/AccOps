{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4217a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from call_count import *\n",
    "def run_inference(batch_sizes=[64, 128, 256, 512], num_batches=5):\n",
    "    \"\"\"\n",
    "    Run inference on Triton models and measure operator-level performance\n",
    "    \"\"\"\n",
    "    print(\"Starting inference test with real dataset...\")\n",
    "    \n",
    "    # Define device\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.set_device(device)  # Set default device\n",
    "    torch.cuda.empty_cache()  # Clear unused GPU memory\n",
    "    \n",
    "    # Try to load pre-trained model\n",
    "    try:\n",
    "        model_state_dict = torch.load(\"model.pt\")\n",
    "        print(\"Successfully loaded pre-trained model weights\")\n",
    "        pytorch_model = Net()\n",
    "        pytorch_model.load_state_dict(model_state_dict)\n",
    "    except:\n",
    "        print(\"Could not load pre-trained model, using default weights\")\n",
    "        pytorch_model = Net()\n",
    "    # pytorch_model = Net()\n",
    "    \n",
    "    # Convert model to use Triton implementations\n",
    "    pytorch_model = pytorch_model.to(device).half()\n",
    "    triton_model = convert_pytorch_to_triton_model(pytorch_model)\n",
    "    triton_model = triton_model.to(device).half()\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    triton_model.eval()\n",
    "    \n",
    "    # Create the dataset once (we'll reuse it for different batch sizes)\n",
    "    try:\n",
    "        train_dataset = GDdataset(\"./train_data.csv\")\n",
    "        print(f\"Successfully loaded dataset with {len(train_dataset)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Test for different batch sizes\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\n==== Batch Size: {batch_size} ====\")\n",
    "        \n",
    "        # Store batches for testing - manually shuffle the dataset indices\n",
    "        indices = torch.randperm(len(train_dataset), device='cpu').tolist()\n",
    "        test_batches = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        for i in range(0, min(len(indices), batch_size * num_batches), batch_size):\n",
    "            if batch_count >= num_batches:\n",
    "                break\n",
    "                \n",
    "            # Get a batch of indices\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            if len(batch_indices) < batch_size:\n",
    "                # Skip incomplete batches\n",
    "                continue\n",
    "                \n",
    "            # Collect inputs and targets for this batch\n",
    "            batch_inputs = []\n",
    "            batch_targets = []\n",
    "            for idx in batch_indices:\n",
    "                inp, tgt = train_dataset[idx]\n",
    "                batch_inputs.append(inp)\n",
    "                batch_targets.append(tgt)\n",
    "            \n",
    "            # Stack the tensors into a batch\n",
    "            inputs = torch.stack(batch_inputs).to(device).half()\n",
    "            targets = torch.stack(batch_targets).to(device).half()\n",
    "            \n",
    "            test_batches.append((inputs, targets))\n",
    "            batch_count += 1\n",
    "        \n",
    "        # Ensure we have enough batches\n",
    "        if len(test_batches) < num_batches:\n",
    "            print(f\"Warning: Could only extract {len(test_batches)} batches instead of {num_batches}\")\n",
    "            num_actual_batches = len(test_batches)\n",
    "        else:\n",
    "            num_actual_batches = num_batches\n",
    "        \n",
    "        # Verify data shape\n",
    "        print(f\"Input shape: {test_batches[0][0].shape}\")\n",
    "        \n",
    "        # Warmup\n",
    "        for i in range(min(10, len(test_batches))):\n",
    "            inputs, _ = test_batches[i % len(test_batches)]\n",
    "            with torch.no_grad():\n",
    "                _ = triton_model(inputs)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Reset counters before measurement\n",
    "        reset_counters()\n",
    "        \n",
    "        # Triton model inference with operator-level profiling\n",
    "        with monitor():\n",
    "            for i in range(num_actual_batches):\n",
    "                inputs, targets = test_batches[i]\n",
    "                with torch.no_grad():\n",
    "                    triton_output = triton_model(inputs)\n",
    "                torch.cuda.synchronize()\n",
    "        \n",
    "        # Print operator-level statistics\n",
    "        # print(\"\\nOperator-level performance statistics (top 10 by total time):\")\n",
    "        # print_statistics(sort_by=\"total_time\", top_n=10)\n",
    "        print(\"\\nOperator-level performance statistics\")\n",
    "        print_statistics()\n",
    "        \n",
    "        # Print sample outputs for the last batch\n",
    "        batch_to_print = min(batch_size, triton_output.size(0))\n",
    "        print(f\"\\nSample output (first {min(5, batch_to_print)} from last batch):\")\n",
    "        for i in range(min(5, batch_to_print)):\n",
    "            print(f\"  Sample {i}: Output={triton_output[i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de102a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import types\n",
    "import time\n",
    "import pprint\n",
    "# 创建一个字典来存储调用次数\n",
    "call_count = {}\n",
    "\n",
    "def count_calls(func, module_name=None):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_count_calls(*args, **kwargs):\n",
    "        # module_name = func.__module__ if hasattr(func, '__module__') and func.__module__ else 'torch'\n",
    "        full_name = module_name + '.' + func.__name__\n",
    "        # call_count[full_name] = call_count.get(full_name, 0) + 1\n",
    "        # print(f\"Function {full_name} called {call_count[full_name]} times\")\n",
    "        # return func(*args, **kwargs)\n",
    "        # 记录开始时间\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        # 记录结束时间\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # 计算调用时间\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        if full_name not in call_count:\n",
    "            call_count[full_name] = {\"count\": 0, \"total_time\": 0.0}\n",
    "        \n",
    "        call_count[full_name][\"count\"] += 1\n",
    "        call_count[full_name][\"total_time\"] += elapsed_time\n",
    "        \n",
    "        return result\n",
    "    wrapper_count_calls._is_decorated = True\n",
    "    return wrapper_count_calls\n",
    "\n",
    "def set_new_attr(module, attr_name, attr):\n",
    "    if not hasattr(attr, \"_is_decorated\"):\n",
    "        decorated_attr = count_calls(attr, module.__name__)\n",
    "        decorated_attr._is_decorated = True\n",
    "        setattr(module, attr_name, decorated_attr)\n",
    "\n",
    "# 递归封装所有的包\n",
    "def auto_decorate_module(module, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    \n",
    "    module_name = module.__name__\n",
    "    if module_name in visited:\n",
    "        return\n",
    "    visited.add(module_name)\n",
    "    for attr_name in dir(module):\n",
    "        try:\n",
    "            attr = getattr(module, attr_name)\n",
    "            # if isinstance(attr, types.FunctionType):\n",
    "            if isinstance(attr, types.FunctionType):\n",
    "                set_new_attr(module, attr_name, attr)\n",
    "                # print(f\"Decorated function: {module_name}.{attr_name}\")\n",
    "            elif isinstance(attr, types.ModuleType) and attr.__name__.startswith('torch'):\n",
    "                # print(f\"Descending into module: {attr.__name__}\")\n",
    "                auto_decorate_module(attr, visited)\n",
    "            elif isinstance(attr, type):\n",
    "                # print(f\"Descending into class: {attr.__name__} in {module_name}\")\n",
    "                auto_decorate_class(attr)\n",
    "            elif callable(attr):\n",
    "                set_new_attr(module, attr_name, attr)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "\n",
    "def auto_decorate_class(cls):\n",
    "    for attr_name in dir(cls):\n",
    "        # if attr_name.startswith('__') and attr_name.endswith('__'):\n",
    "        #     continue  # Skip special attributes\n",
    "        try:\n",
    "            attr = getattr(cls, attr_name)\n",
    "            if isinstance(attr, types.FunctionType):\n",
    "                set_new_attr(cls, attr_name, attr)\n",
    "            elif attr_name in ['__add__', '__mul__', '__sub__', '__truediv__', '__matmul__', '__pow__', '__mod__']:\n",
    "                # 特殊处理运算符重载方法\n",
    "                set_new_attr(cls, attr_name, attr)\n",
    "        except (AttributeError, TypeError) as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4eeabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swat\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import dlpy\n",
    "from dlpy import Sequential\n",
    "from dlpy import *\n",
    "from dlpy.model import TextParms\n",
    "from dlpy.blocks import Bidirectional\n",
    "from dlpy.applications import TextClassification\n",
    "from dlpy.network import *\n",
    "from dlpy.utils import *\n",
    "from dlpy.applications import *\n",
    "from dlpy.model import *\n",
    "from dlpy.images import *\n",
    "from dlpy.layers import *\n",
    "cashost = 'sas-cas-server-default-client'\n",
    "conn = swat.CAS(cashost, 5570, password=os.environ.get('ACCESS_TOKEN'))\n",
    "\n",
    "conn.loadTable(path='jixie_train_data.csv', casout={'name': 'jixie_train_data', 'caslib': 'casuser'}, importOptions={'fileType': 'csv'})\n",
    "tb = conn.CASTable('jixie_train_data', caslib='casuser')\n",
    "tb.shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
